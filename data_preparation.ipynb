{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ],
   "id": "86bb594c73724aa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TARGET_SUBREDDITS = [\n",
    "    'localllama', 'characterai', 'sillytavernai', 'janitorai_official',\n",
    "    'myboyfriendisai', 'chatgpt', 'artificialinteligence', 'singularity',\n",
    "    'fanfiction', 'lonely'\n",
    "]\n",
    "TARGET_SUBREDDITS_LOWER = [s.lower() for s in TARGET_SUBREDDITS]\n",
    "\n",
    "DATA_DIRECTORY = 'filtered_data'"
   ],
   "id": "e3af1f234d8e3689",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_submissions_df(df_raw):\n",
    "\n",
    "    # considering that the data that will undergo this preprocessing has\n",
    "    # already been filtered by the necessary subreddits using reddit_zst_filter,\n",
    "    # this is more of an additional security measure in case of unexpected situations\n",
    "    df_filtered = df_raw[df_raw['subreddit'].str.lower().isin(TARGET_SUBREDDITS_LOWER)].copy()\n",
    "\n",
    "    essential_rs_cols = [\n",
    "        'id',              # unique id for the submission\n",
    "        'created_utc',     # timestamp\n",
    "        'author',          # user who posted\n",
    "        'subreddit',       # subreddit it was posted in\n",
    "        'title',           # the title of the post\n",
    "        'selftext',        # the body text (if a self-post)\n",
    "        'score',           # upvotes/downvotes\n",
    "        'num_comments',    # how many comments it sparked\n",
    "        'over_18',         # NSFW check is relevant to our topic\n",
    "        'url',             # url if it's a link post\n",
    "        'permalink'        # permanent link to the thread\n",
    "    ]\n",
    "\n",
    "    cols_to_keep = [col for col in essential_rs_cols if col in df_filtered.columns]\n",
    "    df_filtered = df_filtered[cols_to_keep]\n",
    "\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=['id']) # duplicates\n",
    "\n",
    "    # normalization, converting UNIX timestamp to a datetime object\n",
    "    df_filtered['created_utc'] = pd.to_datetime(df_filtered['created_utc'], unit='s')\n",
    "\n",
    "    # normalization of common null/deleted text values to numpy.NaN\n",
    "    # this ensures '[removed]' isn't counted as a word in text analysis\n",
    "    null_values = ['[removed]', '[deleted]', '']\n",
    "    df_filtered['selftext'] = df_filtered['selftext'].replace(null_values, np.nan)\n",
    "\n",
    "    # normalization for deleted authors\n",
    "    df_filtered['author'] = df_filtered['author'].replace('[deleted]', np.nan)\n",
    "\n",
    "    return df_filtered"
   ],
   "id": "8ac37fece095e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_comments_df(df_raw):\n",
    "\n",
    "    df_filtered = df_raw[df_raw['subreddit'].str.lower().isin(TARGET_SUBREDDITS_LOWER)].copy()\n",
    "\n",
    "    essential_rc_cols = [\n",
    "        'id',              # unique id\n",
    "        'link_id',         # id of the parent submission (links to RS 'id')\n",
    "        'parent_id',       # id of the parent comment (for threading)\n",
    "        'created_utc',     # timestamp\n",
    "        'author',          # user who commented\n",
    "        'subreddit',       # subreddit\n",
    "        'body',            # the text of the comment\n",
    "        'score'            # upvotes/downvotes\n",
    "    ]\n",
    "\n",
    "    cols_to_keep = [col for col in essential_rc_cols if col in df_filtered.columns]\n",
    "    df_filtered = df_filtered[cols_to_keep]\n",
    "\n",
    "    # duplicates\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=['id'])\n",
    "\n",
    "    # normalization\n",
    "    df_filtered['created_utc'] = pd.to_datetime(df_filtered['created_utc'], unit='s')\n",
    "\n",
    "    null_values = ['[removed]', '[deleted]', '']\n",
    "    df_filtered['body'] = df_filtered['body'].replace(null_values, np.nan)\n",
    "\n",
    "    df_filtered['author'] = df_filtered['author'].replace('[deleted]', np.nan)\n",
    "\n",
    "    # pushshift data has prefixes like 't3_' (submission) or 't1_' (comment)\n",
    "    # we remove these to get the clean ID for merging\n",
    "    df_filtered['link_id'] = df_filtered['link_id'].str.replace('t3_', '')\n",
    "    df_filtered['parent_id'] = df_filtered['parent_id'].str.replace(r't[13]_', '', regex=True)\n",
    "\n",
    "    return df_filtered"
   ],
   "id": "b1a0ab304eebd804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  1. process all RS files\n",
    "rs_files = glob.glob(os.path.join(DATA_DIRECTORY, \"RS_*.parquet\"))\n",
    "all_rs_dfs = []\n",
    "\n",
    "print(f\"\\nFound {len(rs_files)} RS files\")\n",
    "for f in rs_files:\n",
    "    print(f\"  Processing {f}\")\n",
    "    try:\n",
    "        df_raw = pd.read_parquet(f)\n",
    "        df_clean = clean_submissions_df(df_raw)\n",
    "        all_rs_dfs.append(df_clean)\n",
    "        print(f\"    -> Found {len(df_clean)} relevant submissions\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR processing {f}: {e}\")\n",
    "\n",
    "# 1.1. combine into one DF\n",
    "final_rs_df = pd.concat(all_rs_dfs, ignore_index=True)\n",
    "final_rs_df = final_rs_df.drop_duplicates(subset=['id']) # final de-dupe in case a post appears in two monthly files\n",
    "\n",
    "print(f\"Shape: {final_rs_df.shape}\")\n",
    "print(final_rs_df.info())\n",
    "\n",
    "final_rs_df.to_parquet('all_submissions_cleaned.parquet', index=False)"
   ],
   "id": "1e73bbb7cf8b1e3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. process all RC files\n",
    "rc_files = glob.glob(os.path.join(DATA_DIRECTORY, \"RC_*.parquet\"))\n",
    "all_rc_dfs = []\n",
    "\n",
    "print(f\"\\nFound {len(rc_files)} RC files\")\n",
    "for f in rc_files:\n",
    "    print(f\"  Processing {f}\")\n",
    "    try:\n",
    "        df_raw = pd.read_parquet(f)\n",
    "        df_clean = clean_comments_df(df_raw)\n",
    "        all_rc_dfs.append(df_clean)\n",
    "        print(f\"    -> Found {len(df_clean)} relevant comments\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR processing {f}: {e}\")\n",
    "\n",
    "# 2.1. combine into one DF\n",
    "final_rc_df = pd.concat(all_rc_dfs, ignore_index=True)\n",
    "final_rc_df = final_rc_df.drop_duplicates(subset=['id'])\n",
    "\n",
    "print(f\"Shape: {final_rc_df.shape}\")\n",
    "print(final_rc_df.info())\n",
    "\n",
    "final_rc_df.to_parquet('all_comments_cleaned.parquet', index=False)"
   ],
   "id": "299f2643cd89c5be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ba81fe1e46074f0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
